{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LaiAd-WjDyj",
        "outputId": "c89d736d-5fda-46ce-ded3-1d3fa8c90eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        Conference    Height    Weight       Tgt       Rec  \\\n",
            "Conference                1.000000 -0.431859 -0.293086  0.037684  0.045610   \n",
            "Height                   -0.431859  1.000000  0.691509 -0.259134 -0.308766   \n",
            "Weight                   -0.293086  0.691509  1.000000 -0.141399 -0.183518   \n",
            "Tgt                       0.037684 -0.259134 -0.141399  1.000000  0.973613   \n",
            "Rec                       0.045610 -0.308766 -0.183518  0.973613  1.000000   \n",
            "Rec%                      0.097910 -0.336257 -0.284447  0.257648  0.394980   \n",
            "Yds                      -0.045530 -0.244504 -0.139225  0.945851  0.927332   \n",
            "Yd/rec                   -0.152215  0.009258 -0.129605 -0.120618 -0.192980   \n",
            "TD                       -0.181834  0.003082  0.027987  0.819303  0.746868   \n",
            "OFF(Grade)               -0.178202 -0.299262 -0.195790  0.661175  0.693313   \n",
            "RECV(Grade)              -0.176990 -0.312759 -0.204155  0.668164  0.699123   \n",
            "DROP(Grade)              -0.040618 -0.360133 -0.287850  0.319552  0.429471   \n",
            "FUMB(Grade)               0.118384 -0.246506 -0.261602  0.069370  0.085693   \n",
            "#PassRoutes               0.063265 -0.188352 -0.147403  0.924476  0.901162   \n",
            "#Recv Routes              0.064960 -0.194133 -0.153659  0.925815  0.902024   \n",
            "Recv Route%               0.051795 -0.103082 -0.131767 -0.169114 -0.177187   \n",
            "Passing Snaps Blocking    0.248890 -0.247106 -0.066870  0.273657  0.316940   \n",
            "PB%                       0.202788 -0.274961 -0.085528  0.224164  0.271770   \n",
            "#Slot Snaps               0.248806 -0.585203 -0.434241  0.470721  0.459846   \n",
            "SLT%                      0.224528 -0.582688 -0.475914  0.101399  0.111473   \n",
            "#Wide snaps              -0.132911  0.268090  0.188163  0.591981  0.573317   \n",
            "Wide%                    -0.228184  0.589952  0.473894 -0.106072 -0.120131   \n",
            "#INL Snaps                0.110093 -0.293317 -0.113765  0.276951  0.337658   \n",
            "INL%                     -0.185838  0.161679  0.315777 -0.089283 -0.066171   \n",
            "YAC                      -0.034129 -0.311158 -0.206242  0.872312  0.896627   \n",
            "YAC/REC                  -0.099899 -0.304772 -0.363613  0.028824  0.046615   \n",
            "Y/RR                     -0.186259 -0.226920 -0.149555  0.415891  0.425752   \n",
            "ADOT                      0.013019  0.270131  0.122686 -0.191498 -0.316960   \n",
            "LNG                      -0.079477 -0.259138 -0.272499  0.569279  0.556186   \n",
            "#Drops                    0.115250 -0.009159  0.019953  0.625564  0.530341   \n",
            "Drop%                     0.017948  0.316059  0.349649 -0.295505 -0.362746   \n",
            "Cont Tgts                 0.025247 -0.010896  0.125112  0.799745  0.701925   \n",
            "Cont Catches              0.051185 -0.027707  0.136130  0.641568  0.570331   \n",
            "REC INT                   0.007477  0.000492  0.038611  0.533600  0.481882   \n",
            "FUMB                      0.007280  0.062603  0.107017  0.156451  0.137414   \n",
            "Miss Tackles Fcd         -0.120816 -0.139039 -0.125318  0.735189  0.740997   \n",
            "1st Downs                -0.043748 -0.218547 -0.133721  0.960163  0.959130   \n",
            "PRTG TG+                  0.143788 -0.106594 -0.056131  0.148544  0.209831   \n",
            "Score                    -0.001544 -0.292381 -0.185063  0.986640  0.967576   \n",
            "Conf factor               0.135054 -0.308582 -0.179907  0.966795  0.944165   \n",
            "Transfer Grade            0.057369  0.159000  0.089181 -0.576903 -0.521020   \n",
            "\n",
            "                            Rec%       Yds    Yd/rec        TD  OFF(Grade)  \\\n",
            "Conference              0.097910 -0.045530 -0.152215 -0.181834   -0.178202   \n",
            "Height                 -0.336257 -0.244504  0.009258  0.003082   -0.299262   \n",
            "Weight                 -0.284447 -0.139225 -0.129605  0.027987   -0.195790   \n",
            "Tgt                     0.257648  0.945851 -0.120618  0.819303    0.661175   \n",
            "Rec                     0.394980  0.927332 -0.192980  0.746868    0.693313   \n",
            "Rec%                    1.000000  0.306438 -0.203290  0.106651    0.396760   \n",
            "Yds                     0.306438  1.000000  0.092136  0.861817    0.758039   \n",
            "Yd/rec                 -0.203290  0.092136  1.000000  0.179682    0.237063   \n",
            "TD                      0.106651  0.861817  0.179682  1.000000    0.591229   \n",
            "OFF(Grade)              0.396760  0.758039  0.237063  0.591229    1.000000   \n",
            "RECV(Grade)             0.381740  0.766941  0.240774  0.603118    0.991653   \n",
            "DROP(Grade)             0.650206  0.346020 -0.089191  0.162677    0.603778   \n",
            "FUMB(Grade)             0.096619  0.076883  0.070224  0.030230    0.078452   \n",
            "#PassRoutes             0.306418  0.873590 -0.121816  0.753138    0.489542   \n",
            "#Recv Routes            0.304286  0.874285 -0.122997  0.753343    0.490094   \n",
            "Recv Route%            -0.143012 -0.200459  0.075018 -0.178560   -0.114892   \n",
            "Passing Snaps Blocking  0.181784  0.243788 -0.128961  0.079159    0.268496   \n",
            "PB%                     0.179200  0.178312 -0.154296  0.024827    0.252327   \n",
            "#Slot Snaps             0.159675  0.365248 -0.188944  0.140162    0.265934   \n",
            "SLT%                    0.184960  0.003638 -0.189482 -0.183787    0.099029   \n",
            "#Wide snaps             0.188140  0.622658  0.025612  0.677785    0.294851   \n",
            "Wide%                  -0.188019 -0.005855  0.199179  0.187849   -0.108313   \n",
            "#INL Snaps              0.183235  0.289068 -0.129733  0.072414    0.310060   \n",
            "INL%                   -0.228433 -0.088201 -0.153436 -0.081260   -0.046077   \n",
            "YAC                     0.335209  0.915215  0.005597  0.727596    0.717866   \n",
            "YAC/REC                 0.087233  0.142251  0.601512  0.095361    0.254300   \n",
            "Y/RR                    0.376275  0.602743  0.526102  0.485201    0.816650   \n",
            "ADOT                   -0.488268 -0.081580  0.507403  0.095438   -0.120298   \n",
            "LNG                     0.280811  0.734421  0.462508  0.611788    0.660103   \n",
            "#Drops                 -0.074711  0.551237 -0.003267  0.540264    0.127554   \n",
            "Drop%                  -0.636152 -0.326785 -0.143076 -0.164009   -0.563125   \n",
            "Cont Tgts               0.039400  0.750113  0.022726  0.799287    0.478100   \n",
            "Cont Catches            0.077364  0.598127  0.017637  0.661149    0.444860   \n",
            "REC INT                 0.045205  0.490941 -0.027192  0.417955    0.344528   \n",
            "FUMB                    0.028211  0.109336 -0.149747  0.093307    0.020842   \n",
            "Miss Tackles Fcd        0.255634  0.748006 -0.006338  0.657787    0.575164   \n",
            "1st Downs               0.330694  0.960382 -0.049781  0.829846    0.710147   \n",
            "PRTG TG+                0.204410  0.063775 -0.398718 -0.021662    0.037755   \n",
            "Score                   0.288878  0.963196 -0.015304  0.827462    0.731725   \n",
            "Conf factor             0.290756  0.931121 -0.015860  0.783508    0.699353   \n",
            "Transfer Grade          0.015100 -0.614539 -0.221938 -0.556315   -0.437825   \n",
            "\n",
            "                        ...  Cont Tgts  Cont Catches   REC INT      FUMB  \\\n",
            "Conference              ...   0.025247      0.051185  0.007477  0.007280   \n",
            "Height                  ...  -0.010896     -0.027707  0.000492  0.062603   \n",
            "Weight                  ...   0.125112      0.136130  0.038611  0.107017   \n",
            "Tgt                     ...   0.799745      0.641568  0.533600  0.156451   \n",
            "Rec                     ...   0.701925      0.570331  0.481882  0.137414   \n",
            "Rec%                    ...   0.039400      0.077364  0.045205  0.028211   \n",
            "Yds                     ...   0.750113      0.598127  0.490941  0.109336   \n",
            "Yd/rec                  ...   0.022726      0.017637 -0.027192 -0.149747   \n",
            "TD                      ...   0.799287      0.661149  0.417955  0.093307   \n",
            "OFF(Grade)              ...   0.478100      0.444860  0.344528  0.020842   \n",
            "RECV(Grade)             ...   0.480355      0.449594  0.327134  0.014247   \n",
            "DROP(Grade)             ...   0.087648      0.156564  0.146828 -0.012690   \n",
            "FUMB(Grade)             ...  -0.010720     -0.023944  0.021200 -0.764755   \n",
            "#PassRoutes             ...   0.745425      0.586777  0.506255  0.131944   \n",
            "#Recv Routes            ...   0.742158      0.582922  0.506578  0.128836   \n",
            "Recv Route%             ...  -0.217041     -0.195920 -0.094382 -0.106429   \n",
            "Passing Snaps Blocking  ...   0.246830      0.309050  0.303008  0.009420   \n",
            "PB%                     ...   0.159455      0.235783  0.229929 -0.032070   \n",
            "#Slot Snaps             ...   0.170698      0.035392  0.256892  0.001349   \n",
            "SLT%                    ...  -0.139990     -0.172345  0.033934 -0.120879   \n",
            "#Wide snaps             ...   0.643712      0.579209  0.322286  0.139295   \n",
            "Wide%                   ...   0.141585      0.166899 -0.039673  0.125723   \n",
            "#INL Snaps              ...   0.092462      0.057237  0.101750 -0.059685   \n",
            "INL%                    ...  -0.111007     -0.093225 -0.121462 -0.103333   \n",
            "YAC                     ...   0.579765      0.419223  0.480070  0.124631   \n",
            "YAC/REC                 ...  -0.043461     -0.076786  0.040530 -0.043967   \n",
            "Y/RR                    ...   0.327160      0.309095  0.235123  0.041081   \n",
            "ADOT                    ...   0.064881      0.043438 -0.015017 -0.073907   \n",
            "LNG                     ...   0.390031      0.339812  0.385890 -0.061465   \n",
            "#Drops                  ...   0.624246      0.422717  0.321248  0.209780   \n",
            "Drop%                   ...  -0.126838     -0.168861 -0.134418  0.023165   \n",
            "Cont Tgts               ...   1.000000      0.893272  0.537412  0.192438   \n",
            "Cont Catches            ...   0.893272      1.000000  0.480876  0.131959   \n",
            "REC INT                 ...   0.537412      0.480876  1.000000  0.185444   \n",
            "FUMB                    ...   0.192438      0.131959  0.185444  1.000000   \n",
            "Miss Tackles Fcd        ...   0.475847      0.289819  0.405192  0.320540   \n",
            "1st Downs               ...   0.739357      0.574567  0.427991  0.148537   \n",
            "PRTG TG+                ...  -0.037196      0.010135  0.201067 -0.017735   \n",
            "Score                   ...   0.771714      0.613396  0.529981  0.158997   \n",
            "Conf factor             ...   0.775159      0.651563  0.524874  0.119922   \n",
            "Transfer Grade          ...  -0.544534     -0.465020 -0.155155 -0.037755   \n",
            "\n",
            "                        Miss Tackles Fcd  1st Downs  PRTG TG+     Score  \\\n",
            "Conference                     -0.120816  -0.043748  0.143788 -0.001544   \n",
            "Height                         -0.139039  -0.218547 -0.106594 -0.292381   \n",
            "Weight                         -0.125318  -0.133721 -0.056131 -0.185063   \n",
            "Tgt                             0.735189   0.960163  0.148544  0.986640   \n",
            "Rec                             0.740997   0.959130  0.209831  0.967576   \n",
            "Rec%                            0.255634   0.330694  0.204410  0.288878   \n",
            "Yds                             0.748006   0.960382  0.063775  0.963196   \n",
            "Yd/rec                         -0.006338  -0.049781 -0.398718 -0.015304   \n",
            "TD                              0.657787   0.829846 -0.021662  0.827462   \n",
            "OFF(Grade)                      0.575164   0.710147  0.037755  0.731725   \n",
            "RECV(Grade)                     0.576822   0.717118  0.057303  0.737399   \n",
            "DROP(Grade)                     0.214233   0.344618  0.321725  0.357471   \n",
            "FUMB(Grade)                    -0.072463   0.048892  0.094023  0.067975   \n",
            "#PassRoutes                     0.647964   0.895302  0.120007  0.895701   \n",
            "#Recv Routes                    0.650569   0.895864  0.119218  0.897256   \n",
            "Recv Route%                    -0.082407  -0.201237 -0.134558 -0.138621   \n",
            "Passing Snaps Blocking          0.045379   0.191224  0.271381  0.266060   \n",
            "PB%                             0.027014   0.130697  0.335416  0.218669   \n",
            "#Slot Snaps                     0.356675   0.379031  0.055336  0.460663   \n",
            "SLT%                            0.038226   0.013934  0.030680  0.104971   \n",
            "#Wide snaps                     0.398738   0.635846  0.066154  0.570030   \n",
            "Wide%                          -0.035836  -0.014315 -0.053072 -0.108980   \n",
            "#INL Snaps                      0.071969   0.295380 -0.001773  0.266688   \n",
            "INL%                           -0.089669  -0.077067 -0.088966 -0.109015   \n",
            "YAC                             0.861430   0.884264  0.136316  0.913675   \n",
            "YAC/REC                         0.239097   0.069188 -0.159719  0.169705   \n",
            "Y/RR                            0.410027   0.485506 -0.111983  0.518333   \n",
            "ADOT                           -0.253039  -0.177572 -0.272516 -0.213096   \n",
            "LNG                             0.476797   0.600572  0.044382  0.631557   \n",
            "#Drops                          0.550045   0.579899 -0.133293  0.600771   \n",
            "Drop%                          -0.204144  -0.314918 -0.162267 -0.349200   \n",
            "Cont Tgts                       0.475847   0.739357 -0.037196  0.771714   \n",
            "Cont Catches                    0.289819   0.574567  0.010135  0.613396   \n",
            "REC INT                         0.405192   0.427991  0.201067  0.529981   \n",
            "FUMB                            0.320540   0.148537 -0.017735  0.158997   \n",
            "Miss Tackles Fcd                1.000000   0.770511  0.059168  0.787936   \n",
            "1st Downs                       0.770511   1.000000  0.066186  0.963218   \n",
            "PRTG TG+                        0.059168   0.066186  1.000000  0.117388   \n",
            "Score                           0.787936   0.963218  0.117388  1.000000   \n",
            "Conf factor                     0.700639   0.926014  0.129142  0.971629   \n",
            "Transfer Grade                 -0.489705  -0.614096  0.601251 -0.593644   \n",
            "\n",
            "                        Conf factor  Transfer Grade  \n",
            "Conference                 0.135054        0.057369  \n",
            "Height                    -0.308582        0.159000  \n",
            "Weight                    -0.179907        0.089181  \n",
            "Tgt                        0.966795       -0.576903  \n",
            "Rec                        0.944165       -0.521020  \n",
            "Rec%                       0.290756        0.015100  \n",
            "Yds                        0.931121       -0.614539  \n",
            "Yd/rec                    -0.015860       -0.221938  \n",
            "TD                         0.783508       -0.556315  \n",
            "OFF(Grade)                 0.699353       -0.437825  \n",
            "RECV(Grade)                0.704455       -0.434763  \n",
            "DROP(Grade)                0.345437        0.047432  \n",
            "FUMB(Grade)                0.105303       -0.062559  \n",
            "#PassRoutes                0.886009       -0.542208  \n",
            "#Recv Routes               0.886919       -0.543454  \n",
            "Recv Route%               -0.146121        0.038276  \n",
            "Passing Snaps Blocking     0.318672        0.016332  \n",
            "PB%                        0.267575        0.088255  \n",
            "#Slot Snaps                0.474752       -0.282548  \n",
            "SLT%                       0.117848       -0.067629  \n",
            "#Wide snaps                0.546164       -0.352320  \n",
            "Wide%                     -0.125619        0.055133  \n",
            "#INL Snaps                 0.318113       -0.208081  \n",
            "INL%                      -0.113307        0.029403  \n",
            "YAC                        0.856151       -0.513424  \n",
            "YAC/REC                    0.143083       -0.138982  \n",
            "Y/RR                       0.508273       -0.372041  \n",
            "ADOT                      -0.186541       -0.084256  \n",
            "LNG                        0.606255       -0.379187  \n",
            "#Drops                     0.592301       -0.508375  \n",
            "Drop%                     -0.347353        0.018325  \n",
            "Cont Tgts                  0.775159       -0.544534  \n",
            "Cont Catches               0.651563       -0.465020  \n",
            "REC INT                    0.524874       -0.155155  \n",
            "FUMB                       0.119922       -0.037755  \n",
            "Miss Tackles Fcd           0.700639       -0.489705  \n",
            "1st Downs                  0.926014       -0.614096  \n",
            "PRTG TG+                   0.129142        0.601251  \n",
            "Score                      0.971629       -0.593644  \n",
            "Conf factor                1.000000       -0.588206  \n",
            "Transfer Grade            -0.588206        1.000000  \n",
            "\n",
            "[41 rows x 41 columns]\n",
            "(59, 1)\n",
            "(59, 39)\n",
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 75.2128 - val_accuracy: 0.0000e+00 - val_loss: 73.0363\n",
            "Epoch 2/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0000e+00 - loss: 75.1263 - val_accuracy: 0.0000e+00 - val_loss: 72.9400\n",
            "Epoch 3/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.0000e+00 - loss: 75.0375 - val_accuracy: 0.0000e+00 - val_loss: 72.8416\n",
            "Epoch 4/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0000e+00 - loss: 74.9461 - val_accuracy: 0.0000e+00 - val_loss: 72.7409\n",
            "Epoch 5/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0000e+00 - loss: 74.8512 - val_accuracy: 0.0000e+00 - val_loss: 72.6402\n",
            "Epoch 6/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0000e+00 - loss: 74.7532 - val_accuracy: 0.0000e+00 - val_loss: 72.5385\n",
            "Epoch 7/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0000e+00 - loss: 74.6520 - val_accuracy: 0.0000e+00 - val_loss: 72.4347\n",
            "Epoch 8/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.0000e+00 - loss: 74.5470 - val_accuracy: 0.0000e+00 - val_loss: 72.3276\n",
            "Epoch 9/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0000e+00 - loss: 74.4378 - val_accuracy: 0.0000e+00 - val_loss: 72.2176\n",
            "Epoch 10/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 74.3244 - val_accuracy: 0.0000e+00 - val_loss: 72.1039\n",
            "Epoch 11/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0000e+00 - loss: 74.2070 - val_accuracy: 0.0000e+00 - val_loss: 71.9845\n",
            "Epoch 12/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 74.0845 - val_accuracy: 0.0000e+00 - val_loss: 71.8612\n",
            "Epoch 13/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0000e+00 - loss: 73.9563 - val_accuracy: 0.0000e+00 - val_loss: 71.7305\n",
            "Epoch 14/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.0000e+00 - loss: 73.8221 - val_accuracy: 0.0000e+00 - val_loss: 71.5929\n",
            "Epoch 15/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0000e+00 - loss: 73.6809 - val_accuracy: 0.0000e+00 - val_loss: 71.4457\n",
            "Epoch 16/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0000e+00 - loss: 73.5330 - val_accuracy: 0.0000e+00 - val_loss: 71.2901\n",
            "Epoch 17/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.0000e+00 - loss: 73.3781 - val_accuracy: 0.0000e+00 - val_loss: 71.1270\n",
            "Epoch 18/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0000e+00 - loss: 73.2150 - val_accuracy: 0.0000e+00 - val_loss: 70.9548\n",
            "Epoch 19/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 73.0435 - val_accuracy: 0.0000e+00 - val_loss: 70.7763\n",
            "Epoch 20/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.0000e+00 - loss: 72.8639 - val_accuracy: 0.0000e+00 - val_loss: 70.5902\n",
            "Epoch 21/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.0000e+00 - loss: 72.6752 - val_accuracy: 0.0000e+00 - val_loss: 70.3961\n",
            "Epoch 22/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 72.4773 - val_accuracy: 0.0000e+00 - val_loss: 70.1888\n",
            "Epoch 23/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.0000e+00 - loss: 72.2694 - val_accuracy: 0.0000e+00 - val_loss: 69.9706\n",
            "Epoch 24/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0000e+00 - loss: 72.0505 - val_accuracy: 0.0000e+00 - val_loss: 69.7421\n",
            "Epoch 25/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.0000e+00 - loss: 71.8207 - val_accuracy: 0.0000e+00 - val_loss: 69.5033\n",
            "Epoch 26/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.0000e+00 - loss: 71.5796 - val_accuracy: 0.0000e+00 - val_loss: 69.2536\n",
            "Epoch 27/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0000e+00 - loss: 71.3260 - val_accuracy: 0.0000e+00 - val_loss: 68.9921\n",
            "Epoch 28/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.0000e+00 - loss: 71.0603 - val_accuracy: 0.0000e+00 - val_loss: 68.7149\n",
            "Epoch 29/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.0000e+00 - loss: 70.7819 - val_accuracy: 0.0000e+00 - val_loss: 68.4227\n",
            "Epoch 30/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0000e+00 - loss: 70.4906 - val_accuracy: 0.0000e+00 - val_loss: 68.1192\n",
            "Epoch 31/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0000e+00 - loss: 70.1872 - val_accuracy: 0.0000e+00 - val_loss: 67.8013\n",
            "Epoch 32/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0000e+00 - loss: 69.8711 - val_accuracy: 0.0000e+00 - val_loss: 67.4655\n",
            "Epoch 33/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.0000e+00 - loss: 69.5414 - val_accuracy: 0.0000e+00 - val_loss: 67.1141\n",
            "Epoch 34/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0000e+00 - loss: 69.1982 - val_accuracy: 0.0000e+00 - val_loss: 66.7469\n",
            "Epoch 35/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0000e+00 - loss: 68.8396 - val_accuracy: 0.0000e+00 - val_loss: 66.3655\n",
            "Epoch 36/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 68.4656 - val_accuracy: 0.0000e+00 - val_loss: 65.9691\n",
            "Epoch 37/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0000e+00 - loss: 68.0760 - val_accuracy: 0.0000e+00 - val_loss: 65.5549\n",
            "Epoch 38/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 67.6701 - val_accuracy: 0.0000e+00 - val_loss: 65.1213\n",
            "Epoch 39/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 67.2477 - val_accuracy: 0.0000e+00 - val_loss: 64.6702\n",
            "Epoch 40/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0000e+00 - loss: 66.8079 - val_accuracy: 0.0000e+00 - val_loss: 64.2018\n",
            "Epoch 41/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.0000e+00 - loss: 66.3504 - val_accuracy: 0.0000e+00 - val_loss: 63.7146\n",
            "Epoch 42/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.0000e+00 - loss: 65.8748 - val_accuracy: 0.0000e+00 - val_loss: 63.2078\n",
            "Epoch 43/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0000e+00 - loss: 65.3803 - val_accuracy: 0.0000e+00 - val_loss: 62.6818\n",
            "Epoch 44/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 64.8659 - val_accuracy: 0.0000e+00 - val_loss: 62.1361\n",
            "Epoch 45/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0000e+00 - loss: 64.3318 - val_accuracy: 0.0000e+00 - val_loss: 61.5702\n",
            "Epoch 46/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0000e+00 - loss: 63.7776 - val_accuracy: 0.0000e+00 - val_loss: 60.9836\n",
            "Epoch 47/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0000e+00 - loss: 63.2022 - val_accuracy: 0.0000e+00 - val_loss: 60.3758\n",
            "Epoch 48/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0000e+00 - loss: 62.6056 - val_accuracy: 0.0000e+00 - val_loss: 59.7463\n",
            "Epoch 49/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0000e+00 - loss: 61.9869 - val_accuracy: 0.0000e+00 - val_loss: 59.0945\n",
            "Epoch 50/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.0000e+00 - loss: 61.3457 - val_accuracy: 0.0000e+00 - val_loss: 58.4200\n",
            "Epoch 51/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0000e+00 - loss: 60.6813 - val_accuracy: 0.0000e+00 - val_loss: 57.7212\n",
            "Epoch 52/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.0000e+00 - loss: 59.9933 - val_accuracy: 0.0000e+00 - val_loss: 56.9978\n",
            "Epoch 53/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.0000e+00 - loss: 59.2812 - val_accuracy: 0.0000e+00 - val_loss: 56.2489\n",
            "Epoch 54/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.0000e+00 - loss: 58.5439 - val_accuracy: 0.0000e+00 - val_loss: 55.4746\n",
            "Epoch 55/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.0000e+00 - loss: 57.7809 - val_accuracy: 0.0000e+00 - val_loss: 54.6741\n",
            "Epoch 56/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.0000e+00 - loss: 56.9916 - val_accuracy: 0.0000e+00 - val_loss: 53.8470\n",
            "Epoch 57/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 56.1749 - val_accuracy: 0.0000e+00 - val_loss: 52.9927\n",
            "Epoch 58/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.0000e+00 - loss: 55.3307 - val_accuracy: 0.0000e+00 - val_loss: 52.1098\n",
            "Epoch 59/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0000e+00 - loss: 54.4579 - val_accuracy: 0.0000e+00 - val_loss: 51.1982\n",
            "Epoch 60/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.0000e+00 - loss: 53.5555 - val_accuracy: 0.0000e+00 - val_loss: 50.2563\n",
            "Epoch 61/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.0000e+00 - loss: 52.6231 - val_accuracy: 0.0000e+00 - val_loss: 49.2829\n",
            "Epoch 62/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0000e+00 - loss: 51.6594 - val_accuracy: 0.0000e+00 - val_loss: 48.2789\n",
            "Epoch 63/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0000e+00 - loss: 50.6641 - val_accuracy: 0.0000e+00 - val_loss: 47.2432\n",
            "Epoch 64/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 49.6356 - val_accuracy: 0.0000e+00 - val_loss: 46.1736\n",
            "Epoch 65/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 48.5739 - val_accuracy: 0.0000e+00 - val_loss: 45.0707\n",
            "Epoch 66/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0000e+00 - loss: 47.4785 - val_accuracy: 0.0000e+00 - val_loss: 43.9343\n",
            "Epoch 67/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0000e+00 - loss: 46.3489 - val_accuracy: 0.0000e+00 - val_loss: 42.7598\n",
            "Epoch 68/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0000e+00 - loss: 45.1842 - val_accuracy: 0.0000e+00 - val_loss: 41.5467\n",
            "Epoch 69/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.0000e+00 - loss: 43.9832 - val_accuracy: 0.0000e+00 - val_loss: 40.2973\n",
            "Epoch 70/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0000e+00 - loss: 42.7461 - val_accuracy: 0.0000e+00 - val_loss: 39.0102\n",
            "Epoch 71/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0000e+00 - loss: 41.4721 - val_accuracy: 0.0000e+00 - val_loss: 37.6835\n",
            "Epoch 72/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.0000e+00 - loss: 40.1605 - val_accuracy: 0.0000e+00 - val_loss: 36.3173\n",
            "Epoch 73/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.0000e+00 - loss: 38.8101 - val_accuracy: 0.0000e+00 - val_loss: 34.9110\n",
            "Epoch 74/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 37.4203 - val_accuracy: 0.0000e+00 - val_loss: 33.4649\n",
            "Epoch 75/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0000e+00 - loss: 35.9907 - val_accuracy: 0.0000e+00 - val_loss: 31.9770\n",
            "Epoch 76/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.0000e+00 - loss: 34.5206 - val_accuracy: 0.0000e+00 - val_loss: 30.4472\n",
            "Epoch 77/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 33.0089 - val_accuracy: 0.0000e+00 - val_loss: 28.8756\n",
            "Epoch 78/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0000e+00 - loss: 31.5450 - val_accuracy: 0.0000e+00 - val_loss: 27.2700\n",
            "Epoch 79/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0000e+00 - loss: 30.0792 - val_accuracy: 0.0000e+00 - val_loss: 25.6348\n",
            "Epoch 80/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.0000e+00 - loss: 28.7656 - val_accuracy: 0.0000e+00 - val_loss: 23.9762\n",
            "Epoch 81/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 27.4792 - val_accuracy: 0.0000e+00 - val_loss: 22.6644\n",
            "Epoch 82/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.0000e+00 - loss: 26.3711 - val_accuracy: 0.0000e+00 - val_loss: 21.9053\n",
            "Epoch 83/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0000e+00 - loss: 25.3635 - val_accuracy: 0.0000e+00 - val_loss: 21.1433\n",
            "Epoch 84/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.0000e+00 - loss: 24.3530 - val_accuracy: 0.0000e+00 - val_loss: 20.3775\n",
            "Epoch 85/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.0000e+00 - loss: 23.3976 - val_accuracy: 0.0000e+00 - val_loss: 19.6112\n",
            "Epoch 86/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0000e+00 - loss: 22.4702 - val_accuracy: 0.0000e+00 - val_loss: 19.5765\n",
            "Epoch 87/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.0000e+00 - loss: 21.5911 - val_accuracy: 0.0000e+00 - val_loss: 19.6046\n",
            "Epoch 88/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0000e+00 - loss: 20.7870 - val_accuracy: 0.0000e+00 - val_loss: 19.6275\n",
            "Epoch 89/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 20.1257 - val_accuracy: 0.0000e+00 - val_loss: 19.6453\n",
            "Epoch 90/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0000e+00 - loss: 19.4823 - val_accuracy: 0.0000e+00 - val_loss: 19.6574\n",
            "Epoch 91/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0000e+00 - loss: 18.9488 - val_accuracy: 0.0000e+00 - val_loss: 19.6640\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 17.2319\n",
            "Epoch 1/104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 74.6183\n",
            "Epoch 2/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 74.2543 \n",
            "Epoch 3/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 74.6747 \n",
            "Epoch 4/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 73.6676 \n",
            "Epoch 5/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 73.7891 \n",
            "Epoch 6/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 73.4194 \n",
            "Epoch 7/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 72.5988 \n",
            "Epoch 8/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 72.5821 \n",
            "Epoch 9/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 72.6730 \n",
            "Epoch 10/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 71.6739 \n",
            "Epoch 11/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 71.4105 \n",
            "Epoch 12/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 70.6867 \n",
            "Epoch 13/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 70.5811 \n",
            "Epoch 14/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 70.2637 \n",
            "Epoch 15/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 69.1417 \n",
            "Epoch 16/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 68.8482 \n",
            "Epoch 17/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 68.3096 \n",
            "Epoch 18/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 68.2265 \n",
            "Epoch 19/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 67.5411 \n",
            "Epoch 20/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 66.1625 \n",
            "Epoch 21/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 65.3851 \n",
            "Epoch 22/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 64.9278 \n",
            "Epoch 23/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 63.1757 \n",
            "Epoch 24/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 62.3525 \n",
            "Epoch 25/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 61.1090 \n",
            "Epoch 26/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 59.6569 \n",
            "Epoch 27/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 59.2933\n",
            "Epoch 28/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 57.4745 \n",
            "Epoch 29/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 55.3169\n",
            "Epoch 30/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 53.7223 \n",
            "Epoch 31/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0000e+00 - loss: 52.1014\n",
            "Epoch 32/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 50.3718 \n",
            "Epoch 33/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 48.0528 \n",
            "Epoch 34/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 46.3818 \n",
            "Epoch 35/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 45.0128\n",
            "Epoch 36/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 42.9032 \n",
            "Epoch 37/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 40.1586\n",
            "Epoch 38/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 37.0237 \n",
            "Epoch 39/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 35.2178\n",
            "Epoch 40/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 31.0510 \n",
            "Epoch 41/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 30.2566 \n",
            "Epoch 42/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 28.4491 \n",
            "Epoch 43/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 25.2333 \n",
            "Epoch 44/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 25.0132\n",
            "Epoch 45/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 23.3336  \n",
            "Epoch 46/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 21.1409 \n",
            "Epoch 47/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 20.9163 \n",
            "Epoch 48/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 20.3383 \n",
            "Epoch 49/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 19.7280 \n",
            "Epoch 50/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 19.1085\n",
            "Epoch 51/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 18.8276 \n",
            "Epoch 52/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 19.2766\n",
            "Epoch 53/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 19.0953 \n",
            "Epoch 54/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 18.8693 \n",
            "Epoch 55/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 18.1888 \n",
            "Epoch 56/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 19.2094 \n",
            "Epoch 57/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 18.6963 \n",
            "Epoch 58/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 18.3746\n",
            "Epoch 59/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 17.9327\n",
            "Epoch 60/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 17.6934 \n",
            "Epoch 61/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 17.4385 \n",
            "Epoch 62/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 18.7884  \n",
            "Epoch 63/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 16.5944 \n",
            "Epoch 64/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 16.3132 \n",
            "Epoch 65/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 17.3166 \n",
            "Epoch 66/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 16.5938 \n",
            "Epoch 67/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 15.5002 \n",
            "Epoch 68/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 16.7955 \n",
            "Epoch 69/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 15.3680 \n",
            "Epoch 70/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 16.3945\n",
            "Epoch 71/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 15.9047\n",
            "Epoch 72/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 15.3269\n",
            "Epoch 73/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 15.3723 \n",
            "Epoch 74/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 14.8704 \n",
            "Epoch 75/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 13.9626 \n",
            "Epoch 76/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 14.6052 \n",
            "Epoch 77/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 14.4247 \n",
            "Epoch 78/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 13.9498 \n",
            "Epoch 79/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 14.2223 \n",
            "Epoch 80/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 14.0225 \n",
            "Epoch 81/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 13.4432 \n",
            "Epoch 82/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 13.2433 \n",
            "Epoch 83/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 12.8678 \n",
            "Epoch 84/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 12.2454\n",
            "Epoch 85/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 12.1089\n",
            "Epoch 86/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 11.8542 \n",
            "Epoch 87/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 12.0611 \n",
            "Epoch 88/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 11.7564 \n",
            "Epoch 89/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 11.9025 \n",
            "Epoch 90/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 10.5297\n",
            "Epoch 91/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 10.6356\n",
            "Epoch 92/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 10.4030 \n",
            "Epoch 93/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 10.3678 \n",
            "Epoch 94/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 10.6079 \n",
            "Epoch 95/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 9.5640 \n",
            "Epoch 96/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 9.5297 \n",
            "Epoch 97/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 9.2229 \n",
            "Epoch 98/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 9.2039 \n",
            "Epoch 99/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 8.7807 \n",
            "Epoch 100/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 8.6324 \n",
            "Epoch 101/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 8.1791 \n",
            "Epoch 102/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 8.2666 \n",
            "Epoch 103/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 7.9765 \n",
            "Epoch 104/104\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 7.1223 \n",
            "(29, 39)\n",
            "(None, 39)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "[[88.936745]\n",
            " [89.457085]\n",
            " [82.63074 ]\n",
            " [78.021095]\n",
            " [85.91712 ]\n",
            " [86.384285]\n",
            " [69.626915]\n",
            " [83.626   ]\n",
            " [86.19296 ]\n",
            " [76.98054 ]\n",
            " [74.892586]\n",
            " [76.127205]\n",
            " [82.67979 ]\n",
            " [73.16578 ]\n",
            " [71.29373 ]\n",
            " [78.37568 ]\n",
            " [74.061   ]\n",
            " [80.48286 ]\n",
            " [66.997055]\n",
            " [72.405106]\n",
            " [72.22151 ]\n",
            " [81.83849 ]\n",
            " [84.75977 ]\n",
            " [72.92156 ]\n",
            " [72.80262 ]\n",
            " [81.565636]\n",
            " [82.077156]\n",
            " [63.55986 ]\n",
            " [46.783455]]\n",
            "                    Name  Year         Former School  Conference  Height  \\\n",
            "0      Antwane Wells Jr.  2022        South Carolina           3      73   \n",
            "1             CJ Daniels  2023               Liberty           9      74   \n",
            "2           Loic Fouonji  2022            Texas Tech           5      76   \n",
            "3            Devin Price  2023      Florida Atlantic           6      75   \n",
            "4      Chris Brazzell II  2023                Tulane           6      77   \n",
            "5           Colbie Young  2023                 Miami           7      77   \n",
            "6   Raymond Cottrell Jr.  2023             Texas A&M           3      75   \n",
            "7           Jabre Barber  2023                  Troy           4      70   \n",
            "8        Kelly Akharaiyi  2023                  UTEP           9      73   \n",
            "9           Chimere Dike  2023             Wisconsin           8      73   \n",
            "10        Germie Bernard  2023            Washington           8      73   \n",
            "11           Isaiah Bond  2023               Alabama           3      71   \n",
            "12           Cyrus Allen  2023        Louisiana Tech           9      72   \n",
            "13         Kevin Coleman  2023            Louisville           7      71   \n",
            "14  Ahmari Huggins-Bruce  2023            Louisville           7      70   \n",
            "15         Zavion Thomas  2023     Mississippi State           3      71   \n",
            "16      London Humphreys  2023            Vanderbilt           3      75   \n",
            "17        Elijhah Badger  2022         Arizona State           5      74   \n",
            "18   Michael Jackson III  2023                   USC           8      72   \n",
            "19       Fred Farrier II  2023                   UAB           6      73   \n",
            "20        Matthew Golden  2023               Houston           5      72   \n",
            "21          Robert Lewis  2023         Georgia State           4      71   \n",
            "22           Deion Burks  2023                Purdue           8      71   \n",
            "23        Gage Larvadain  2023            Miami (OH)           2      70   \n",
            "24           Jared Brown  2023      Coastal Carolina           4      72   \n",
            "25        Ja'Mori Maclin  2023           North Texas           6      71   \n",
            "26      Tommy Winton III  2023  East Tennessee State           0      70   \n",
            "27         Dariyan Wiley  2023                Nevada           1      73   \n",
            "28        Jordan Anthony  2023             Texas A&M           3      70   \n",
            "\n",
            "    Weight  Tgt  Rec   Rec%   Yds  ...  REC INT  FUMB  Miss Tackles Fcd  \\\n",
            "0      210   83   64   77.1   911  ...        4     3                19   \n",
            "1      200   62   40   64.5   831  ...        1     0                 8   \n",
            "2      215   42   27   64.3   335  ...        2     2                10   \n",
            "3      205    9    7   77.8   120  ...        1     0                 0   \n",
            "4      195   62   41   66.1   670  ...        0     0                 1   \n",
            "5      215   63   47   74.6   574  ...        0     1                10   \n",
            "6      215    1    1  100.0    13  ...        0     0                 0   \n",
            "7      174   93   61   65.6   841  ...        1     2                 8   \n",
            "8      194   89   47   52.8  1021  ...        2     0                 7   \n",
            "9      200   37   19   51.4   328  ...        1     0                 2   \n",
            "10     203   39   30   76.9   377  ...        0     0                 8   \n",
            "11     182   64   40   62.5   545  ...        1     0                 5   \n",
            "12     177   75   46   61.3   782  ...        0     1                 4   \n",
            "13     180   33   23   69.7   334  ...        2     0                 4   \n",
            "14     170   27   20   74.1   312  ...        1     1                 2   \n",
            "15     190   58   40   69.0   502  ...        2     1                12   \n",
            "16     186   46   21   45.7   437  ...        1     0                 0   \n",
            "17     190  100   70   70.0   863  ...        1     1                25   \n",
            "18     200   21   17   81.0   146  ...        0     0                 0   \n",
            "19     180   30   18   60.0   266  ...        2     0                 3   \n",
            "20     190   63   39   61.9   419  ...        1     0                 8   \n",
            "21     185  103   70   68.0   877  ...        3     0                13   \n",
            "22     195   95   47   49.5   629  ...        6     0                18   \n",
            "23     165   57   36   63.2   615  ...        1     1                 3   \n",
            "24     175   81   59   72.8   742  ...        0     0                27   \n",
            "25     183  100   57   57.0  1006  ...        3     0                14   \n",
            "26     196   56   23   41.1   349  ...        5     0                 3   \n",
            "27     190   51   24   47.1   400  ...        0     1                 2   \n",
            "28     160    4    3   75.0    14  ...        0     0                 0   \n",
            "\n",
            "    1st Downs  PRTG TG+  Penalties  Score  Conf factor  Transfer Grade  \\\n",
            "0          34  1.714901      4 (0)  17.12        17.12       88.132630   \n",
            "1          30  1.440341      3 (0)  13.98        14.87       86.924910   \n",
            "2          16  1.452099      1 (0)   9.55         9.27       85.171090   \n",
            "3           5  1.077453      0 (0)   4.64         4.84       84.563484   \n",
            "4          35  1.480248      2 (1)  12.79        13.32       83.379654   \n",
            "5          28  1.489510      1 (0)  13.20        14.67       82.943730   \n",
            "6           1  1.006317      0 (0)   3.55         3.55       81.365380   \n",
            "7          37  1.861909      1 (0)  17.70        18.06       80.567444   \n",
            "8          35  1.803975      5 (1)  17.26        18.37       78.690400   \n",
            "9          14  1.464824      0 (0)   8.25        10.06       77.104120   \n",
            "10         18  1.339130      1 (0)   9.82        11.98       76.034920   \n",
            "11         24  1.615385      0 (0)  12.91        12.91       75.721650   \n",
            "12         32  1.655594      3 (0)  14.38        15.30       75.479690   \n",
            "13         13  1.339157      0 (0)   8.65         9.61       75.019840   \n",
            "14         12  1.198384      0 (0)   7.48         8.31       74.477844   \n",
            "15         25  1.666667      1 (0)  12.42        12.42       73.906456   \n",
            "16         16  1.461847      0 (0)  10.12        10.12       73.470000   \n",
            "17         41  1.865801      4 (1)  19.44        18.88       73.018456   \n",
            "18          9  1.188341      2 (1)   6.03         7.35       72.770874   \n",
            "19         10  1.414365      0 (0)   7.27         7.57       72.629295   \n",
            "20         27  1.591549      3 (1)  12.91        12.53       72.200410   \n",
            "21         38  1.983763      1 (0)  19.10        19.49       71.805580   \n",
            "22         29  2.374819      0 (0)  17.49        21.33       70.779310   \n",
            "23         24  1.447410      2 (0)  12.85        17.14       70.767914   \n",
            "24         26  1.689949      0 (0)  17.56        17.92       69.309494   \n",
            "25         39  1.864304      1 (0)  18.65        19.43       69.300140   \n",
            "26         13  3.231076      1 (0)  10.85         9.11       68.145200   \n",
            "27         13  1.585534      3 (0)  10.26        11.03       60.788246   \n",
            "28          1  1.050505      0 (0)   3.06         3.06       55.694840   \n",
            "\n",
            "    Predictions1  \n",
            "0      88.936745  \n",
            "1      89.457085  \n",
            "2       82.63074  \n",
            "3      78.021095  \n",
            "4       85.91712  \n",
            "5      86.384285  \n",
            "6      69.626915  \n",
            "7         83.626  \n",
            "8       86.19296  \n",
            "9       76.98054  \n",
            "10     74.892586  \n",
            "11     76.127205  \n",
            "12      82.67979  \n",
            "13      73.16578  \n",
            "14      71.29373  \n",
            "15      78.37568  \n",
            "16        74.061  \n",
            "17      80.48286  \n",
            "18     66.997055  \n",
            "19     72.405106  \n",
            "20      72.22151  \n",
            "21      81.83849  \n",
            "22      84.75977  \n",
            "23      72.92156  \n",
            "24      72.80262  \n",
            "25     81.565636  \n",
            "26     82.077156  \n",
            "27      63.55986  \n",
            "28     46.783455  \n",
            "\n",
            "[29 rows x 47 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Read training and test data\n",
        "test = pd.read_csv(\"/content/Transfer data project - 2024 WRs (4).csv\")\n",
        "train = pd.read_csv(\"/content/Transfer data project - SEC Transfer WRS Before (4).csv\")\n",
        "\n",
        "test = test.drop(columns = [\"Name\", \"Year\", \"Former School\", \"PBLK(Grade)\", \"Penalties\"])\n",
        "train = train.drop(columns = [\"Name\", \"Year\", \"Prev School\", \"PBLK(Grade)\", \"Penalties\"])\n",
        "\n",
        "\n",
        "# Correlation matrix for initial data exploration\n",
        "print(train.corr())\n",
        "\n",
        "# Clearing environment\n",
        "del train, test\n",
        "\n",
        "# Load Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Read training data again\n",
        "train = pd.read_csv(\"/content/Transfer data project - SEC Transfer WRS Before (4).csv\")\n",
        "\n",
        "\n",
        "# Scaling data\n",
        "indices_to_scale = [3,4,5,6,7,8,9,10,11,12,13,14,15,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,44]\n",
        "indices_to_scale_test = [1,2,3,4,5,6,7,8,9,10,11]\n",
        "#print(indices_to_scale)\n",
        "scaler = MinMaxScaler()\n",
        "train_scaled = train.copy()  # Make a copy of the original DataFrame\n",
        "train_scaled.iloc[:, indices_to_scale] = scaler.fit_transform(train.iloc[:, indices_to_scale])\n",
        "\n",
        "\n",
        "X = train_scaled.iloc[:, indices_to_scale]\n",
        "\n",
        "y = train_scaled.iloc[:, [45]]\n",
        "\n",
        "#x = train_scaled.iloc[:, indices_to_scale_test]\n",
        "\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "# Creating Keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=X.shape[1]))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=y.shape[1], activation='linear'))  # Adjusted to match the number of columns in y\n",
        "\n",
        "# Compiling model\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Splitting into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "# Defining early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Training model\n",
        "history = model.fit(X_train, y_train,  batch_size=64, epochs=500, validation_split=0.1, callbacks=[early_stopping])\n",
        "\n",
        "# Store number of epochs for future reference\n",
        "num_epochs_run = len(history.history['val_loss'])\n",
        "\n",
        "# Testing model\n",
        "model.evaluate(X_test, y_test)\n",
        "\n",
        "# Creating Keras model\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(units=64, activation='relu', input_dim=X.shape[1]))\n",
        "final_model.add(Dense(units=32, activation='relu'))\n",
        "final_model.add(Dense(units=y.shape[1], activation='linear'))  # Adjusted to match the number of columns in y\n",
        "\n",
        "# Compiling model\n",
        "final_model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training final model\n",
        "final_model.fit(X, y, epochs=int(num_epochs_run*1.15), batch_size=32)\n",
        "\n",
        "# Read actual testing data\n",
        "final_test = pd.read_csv(\"/content/Transfer data project - 2024 WRs (4).csv\")\n",
        "\n",
        "\n",
        "# Scale testing data\n",
        "final_test_scaled = scaler.transform(final_test.iloc[:, indices_to_scale])\n",
        "\n",
        "# Predict new activity ids using model\n",
        "print(final_test_scaled.shape)\n",
        "print(model.input_shape)\n",
        "predictions = final_model.predict(final_test_scaled)\n",
        "#for p in predictions:\n",
        "  #fp = p.replace(\" \", \",\")\n",
        "  #print(fp)\n",
        "#p2d = predictions.reshape(-1, 2)\n",
        "#print(p2d)\n",
        "string_list = [' '.join(map(str, sublist)) for sublist in predictions]\n",
        "\n",
        "# Join the strings with newline character '\\n' to separate them\n",
        "final_string = '\\n'.join(string_list)\n",
        "\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "# Assign values to new dataframe\n",
        "final_test['Predictions1'] = (string_list)\n",
        "#final_test['Predictions2'] = np.nonzero(predictions)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(final_test)\n",
        "\n",
        "# Exporting dataset\n",
        "final_test.to_csv(\"ACC_test_with_predictions.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BijuzX48XMr1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}